{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Notebook (Catalyst)\n",
    "\n",
    "### Experiments\n",
    "\n",
    "* [x] Efficientnet b5 **PL:0.652**\n",
    "* [x] FP 16，refer to this [catalyst tutorial](https://github.com/catalyst-team/catalyst/blob/master/examples/notebooks/segmentation-tutorial.ipynb)\n",
    "    * The model will have gradient overflow after 5th epoch, everything else is okay\n",
    "* [x] Saving & Loading from JIT **PL:0.655**\n",
    "* [x] Ensemble\n",
    "* [x] 384x576\n",
    "* [x] polygon convex\n",
    "* [ ] Test the funnel network again\n",
    "* [ ] Ranger optimizer \n",
    "    * [ ] RADAM\n",
    "    * [ ] Look Ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Apex for FP16\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/NVIDIA/apex\n",
    "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n",
    "is_fp16_used = True\n",
    "```\n",
    "\n",
    "### Other Installations\n",
    "\n",
    "```\n",
    "pip install catalyst\n",
    "pip install pretrainedmodels\n",
    "pip install git+https://github.com/qubvel/segmentation_models.pytorch\n",
    "pip install pip pytorch-toolbelt\n",
    "pip install torchvision==0.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our starter kernel is from [this open kernel](https://www.kaggle.com/artgor/segmentation-in-pytorch-using-convenient-tools)\n",
    "\n",
    "## ======== Below is from the original notebook ===========\n",
    "\n",
    "## General information\n",
    "\n",
    "In this kernel I work with the data from Understanding Clouds from Satellite Images competition.\n",
    "```\n",
    "Shallow clouds play a huge role in determining the Earth's climate. They’re also difficult to understand and to represent in climate models. By classifying different types of cloud organization, researchers at Max Planck hope to improve our physical understanding of these clouds, which in turn will help us build better climate models.\n",
    "```\n",
    "\n",
    "So in this competition we are tasked with multiclass segmentation task: finding 4 different cloud patterns in the images. On the other hand, we make predictions for each pair of image and label separately, so this could be treated as 4 binary segmentation tasks.\n",
    "It is important to notice that images (and masks) are `1400 x 2100`, but predicted masks should be `350 x 525`.\n",
    "\n",
    "In this kernel I'll use (or will use in next versions) the following notable libraries:\n",
    "- [albumentations](https://github.com/albu/albumentations): this is a great library for image augmentation which makes it easier and more convenient\n",
    "- [catalyst](https://github.com/catalyst-team/catalyst): this is a great library which makes using PyTorch easier, helps with reprodicibility and contains a lot of useful utils\n",
    "- [segmentation_models_pytorch](https://github.com/qubvel/segmentation_models.pytorch): this is a great library with convenient wrappers for models, losses and other useful things\n",
    "- [pytorch-toolbelt](https://github.com/BloodAxe/pytorch-toolbelt): this is a great library with many useful shortcuts for building pytorch models\n",
    "\n",
    "\n",
    "UPD: Version 35 - changed calculation of optimal threshold and min size\n",
    "![](https://i.imgur.com/EOvz5kd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import collections\n",
    "import time \n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "train_on_gpu = True\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "import albumentations as albu\n",
    "from albumentations import pytorch as AT\n",
    "\n",
    "from catalyst.data import Augmentor\n",
    "from catalyst.dl import utils\n",
    "from catalyst.data.reader import ImageReader, ScalarReader, ReaderCompose, LambdaReader\n",
    "from catalyst.dl.runner import SupervisedRunner\n",
    "from catalyst.contrib.models.segmentation import Unet\n",
    "from catalyst.dl.callbacks import DiceCallback, EarlyStoppingCallback, InferCallback, CheckpointCallback\n",
    "\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configs and Hyper-Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_ucsi import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False # Do we have training?\n",
    "num_epochs = 40 # How many epochs are we going to train?\n",
    "\n",
    "FP16 = False # Do we use half precision?\n",
    "fp16_params = dict(opt_level = \"O2\") if FP16 else None\n",
    "\n",
    "LOAD = False # Do we load a trained weights at the beginning\n",
    "LOAD_PATH = \"cata-eff-b5.pth\" # The model weight path, if we load a trained weights at the begining\n",
    "\n",
    "ENCODER = 'efficientnet-b5' # Encoder model name\n",
    "ENCODER_WEIGHTS = 'imagenet' # Encoder pretrained weights\n",
    "DEVICE = 'cuda' \n",
    "\n",
    "ACTIVATION = None\n",
    "\n",
    "MIN_SIZE_RANGE = 3\n",
    "MIN_SIZE = [0, 100, 1200, 5000,8000, 10000][-MIN_SIZE_RANGE:]\n",
    "\n",
    "INPUT_SIZE = (384,576)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PRED = True\n",
    "# Ensemble Model Path List\n",
    "\n",
    "MODEL_PATHS = [\n",
    "    {\"path\":\"pth/fpn_se_resnext.pth\",\"encoder\":\"se_resnext101_32x4d\"},\n",
    "#     {\"path\":\"pth/senet154_384x576.pth\",\"encoder\":\"senet154\"},\n",
    "]\n",
    "\n",
    "\n",
    "# INFER_BS = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoders\n",
    "Encoder backbone of fpn/unet\n",
    "#### VGG\n",
    "vgg11, vgg13, vgg16, vgg19, vgg11bn, vgg13bn, vgg16bn, vgg19bn,\n",
    "#### DenseNet\n",
    "densenet121, densenet169, densenet201, densenet161, dpn68, dpn98, dpn131,\n",
    "inceptionresnetv2,\n",
    "#### ResNet\n",
    "resnet18, resnet34, resnet50, resnet101, resnet152,\n",
    "resnext50_32x4d, resnext101_32x8d,\n",
    "#### Se ResNet\n",
    "se_resnet50, se_resnet101, se_resnet152,\n",
    "#### Se ResNext\n",
    "se_resnext50_32x4d, se_resnext101_32x4d,\n",
    "#### Se Net\n",
    "senet154,\n",
    "#### Efficient Net\n",
    "efficientnet-b0, efficientnet-b1, efficientnet-b2, efficientnet-b3, efficientnet-b4, efficientnet-b5, efficientnet-b6, efficientnet-b7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "\n",
    "Let's have a look at the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "HOME = Path(os.environ[\"HOME\"])\n",
    "\n",
    "path = HOME/'ucsi'\n",
    "# os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have folders with train and test images, file with train image ids and masks and sample submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f'{path}/train.csv')\n",
    "sub = pd.read_csv(f'{path}/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Label</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0011165.jpg_Fish</td>\n",
       "      <td>264918 937 266318 937 267718 937 269118 937 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0011165.jpg_Flower</td>\n",
       "      <td>1355565 1002 1356965 1002 1358365 1002 1359765...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0011165.jpg_Gravel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0011165.jpg_Sugar</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>002be4f.jpg_Fish</td>\n",
       "      <td>233813 878 235213 878 236613 878 238010 881 23...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image_Label                                      EncodedPixels\n",
       "0    0011165.jpg_Fish  264918 937 266318 937 267718 937 269118 937 27...\n",
       "1  0011165.jpg_Flower  1355565 1002 1356965 1002 1358365 1002 1359765...\n",
       "2  0011165.jpg_Gravel                                                NaN\n",
       "3   0011165.jpg_Sugar                                                NaN\n",
       "4    002be4f.jpg_Fish  233813 878 235213 878 236613 878 238010 881 23..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5546 images in train dataset\n",
      "There are 3698 images in test dataset\n"
     ]
    }
   ],
   "source": [
    "n_train = len(os.listdir(f'{path}/train_images'))\n",
    "n_test = len(os.listdir(f'{path}/test_images'))\n",
    "print(f'There are {n_train} images in train dataset')\n",
    "print(f'There are {n_test} images in test dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flower    5546\n",
       "Sugar     5546\n",
       "Fish      5546\n",
       "Gravel    5546\n",
       "Name: Image_Label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Image_Label'].apply(lambda x: x.split('_')[1]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have ~5.5k images in train dataset and they can have up to 4 masks: Fish, Flower, Gravel and Sugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sugar     3751\n",
       "Gravel    2939\n",
       "Fish      2781\n",
       "Flower    2365\n",
       "Name: Image_Label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train['EncodedPixels'].isnull() == False, 'Image_Label'].apply(lambda x: x.split('_')[1]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    2372\n",
       "3    1560\n",
       "1    1348\n",
       "4     266\n",
       "Name: Image_Label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train['EncodedPixels'].isnull() == False, 'Image_Label'].apply(lambda x: x.split('_')[0]).value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are a lot of empty masks. In fact only 266 images have all four masks. It is important to remember this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = train['Image_Label'].apply(lambda x: x.split('_')[1])\n",
    "train['im_id'] = train['Image_Label'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "\n",
    "sub['label'] = sub['Image_Label'].apply(lambda x: x.split('_')[1])\n",
    "sub['im_id'] = sub['Image_Label'].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the images and the masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize=(25, 16))\n",
    "#for j, im_id in enumerate(np.random.choice(train['im_id'].unique(), 4)):\n",
    "#    for i, (idx, row) in enumerate(train.loc[train['im_id'] == im_id].iterrows()):\n",
    "#        ax = fig.add_subplot(5, 4, j * 4 + i + 1, xticks=[], yticks=[])\n",
    "#        im = Image.open(f\"{path}/train_images/{row['Image_Label'].split('_')[0]}\")\n",
    "#        plt.imshow(im)\n",
    "#       mask_rle = row['EncodedPixels']\n",
    "#        try: # label might not be there!\n",
    "#            mask = rle_decode(mask_rle)\n",
    "#        except:\n",
    "#            mask = np.zeros((1400, 2100))\n",
    "#        plt.imshow(mask, alpha=0.5, cmap='gray')\n",
    "#        ax.set_title(f\"Image: {row['Image_Label'].split('_')[0]}. Label: {row['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that masks can overlap. Also we can see that clouds are really similar to fish, flower and so on. Another important point: masks are often quite big and can have seemingly empty areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for modelling\n",
    "\n",
    "At first, let's create a list of unique image ids and the count of masks for images. This will allow us to make a stratified split based on this count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mask_count = train.loc[train['EncodedPixels'].isnull() == False, 'Image_Label'].apply(lambda x: x.split('_')[0]).value_counts().\\\n",
    "reset_index().rename(columns={'index': 'img_id', 'Image_Label': 'count'})\n",
    "train_ids, valid_ids = train_test_split(id_mask_count['img_id'].values, random_state=42, stratify=id_mask_count['count'], test_size=0.1)\n",
    "test_ids = sub['Image_Label'].apply(lambda x: x.split('_')[0]).drop_duplicates().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring augmentations with albumentations\n",
    "\n",
    "One of important things while working with images is choosing good augmentations. There are a lot of them, let's have a look at augmentations from albumentations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_name = '8242ba0.jpg'\n",
    "#image = get_img(image_name)\n",
    "#mask = make_mask(train, image_name)\n",
    "\n",
    "#visualize(image, mask)\n",
    "\n",
    "# This is how original image and its masks look like. Let's try adding some augmentations\n",
    "\n",
    "#plot_with_augmentation(image, mask, albu.HorizontalFlip(p=1))\n",
    "\n",
    "#plot_with_augmentation(image, mask, albu.VerticalFlip(p=1))\n",
    "\n",
    "#plot_with_augmentation(image, mask, albu.RandomRotate90(p=1))\n",
    "\n",
    "#plot_with_augmentation(image, mask, albu.ElasticTransform(p=1, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03))\n",
    "\n",
    "#plot_with_augmentation(image, mask, albu.GridDistortion(p=1))\n",
    "\n",
    "#plot_with_augmentation(image, mask, albu.OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up data for training in Catalyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.FPN(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=4, \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2ray2c/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:2875: UserWarning:\n",
      "\n",
      "Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_workers = 0\n",
    "bs = 10 if FP16 else 5\n",
    "train_dataset = CloudDataset(df=train, datatype='train', img_ids=train_ids, transforms = get_training_augmentation(), preprocessing=get_preprocessing(preprocessing_fn))\n",
    "valid_dataset = CloudDataset(df=train, datatype='valid', img_ids=valid_ids, transforms = get_validation_augmentation(), preprocessing=get_preprocessing(preprocessing_fn))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=bs, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "loaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"valid\": valid_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logdir = \"./logs/segmentation\"\n",
    "\n",
    "# model, criterion, optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.decoder.parameters(), 'lr': 1e-3}, \n",
    "    {'params': model.encoder.parameters(), 'lr': 5e-4},  # Pretrained section of the model using smaller lr\n",
    "], \n",
    "    weight_decay=3e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.25, patience=2)\n",
    "criterion = smp.utils.losses.BCEDiceLoss(eps=1.)\n",
    "runner = SupervisedRunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD:\n",
    "    model.load_state_dict(torch.load(LOAD_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    runner.train(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loaders=loaders,\n",
    "        callbacks=[DiceCallback(), EarlyStoppingCallback(patience=5, min_delta=0.001)],\n",
    "        logdir=logdir,\n",
    "        num_epochs=num_epochs,\n",
    "        fp16=fp16_params,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    torch.save(runner.model.state_dict(),'pth/final_%s.pth'%(datetime.now().strftime(\"%m%d_%H%M%S\")))\n",
    "\n",
    "    \n",
    "    utils.plot_metrics(\n",
    "        logdir=logdir, \n",
    "        # specify which metrics we want to plot\n",
    "        metrics=[\"loss\", \"dice\", 'lr', '_base/lr']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    print(\"=\"*100)\n",
    "    print(\"Saving to JIT model\")\n",
    "    runner.model = runner.model.eval()\n",
    "    traced = torch.jit.trace(runner.model, torch.rand(2,3,*INPUT_SIZE).cuda())\n",
    "    traced.save(\"jit/jit-cata-eff-b5-v2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring predictions\n",
    "Let's make predictions on validation dataset.\n",
    "\n",
    "At first we need to optimize thresholds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_cb = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ensModel(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super().__init__()\n",
    "        self.models = models\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        res = []\n",
    "        x = x.cuda()\n",
    "        with torch.no_grad():\n",
    "            for m in self.models:\n",
    "                res.append(m(x))\n",
    "        res = torch.stack(res)\n",
    "        return torch.mean(res, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ensemble models\n",
      "======================================================================\n",
      "loading se_resnext101_32x4d from path 'pth/fpn_se_resnext.pth'\n"
     ]
    }
   ],
   "source": [
    "if MODEL_PRED:\n",
    "    print(\"Loading ensemble models\")\n",
    "    print(\"=\"*70)\n",
    "    models = list(loadModel(path = p[\"path\"],encoder = p[\"encoder\"]) for p in MODEL_PATHS)\n",
    "    if torch.cuda.is_available():\n",
    "        models = list(m.cuda() for m in models)\n",
    "    model = ensModel(models)\n",
    "else:\n",
    "    infer_cb.append(CheckpointCallback(resume=f\"{logdir}/checkpoints/best.pth\"),)\n",
    "infer_cb.append(InferCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "555it [00:59,  9.34it/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_pixels = []\n",
    "loaders = {\"infer\": valid_loader}\n",
    "runner.infer(\n",
    "    model=model,\n",
    "    loaders=loaders,\n",
    "    callbacks=infer_cb,\n",
    ")\n",
    "valid_masks = []\n",
    "probabilities = np.zeros((2220, 350, 525))\n",
    "for i, (batch, output) in enumerate(tqdm.tqdm(zip(\n",
    "        valid_dataset, runner.callbacks[0].predictions[\"logits\"]))):\n",
    "    image, mask = batch\n",
    "    for m in mask:\n",
    "        if m.shape != (350, 525):\n",
    "            m = cv2.resize(m, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n",
    "        valid_masks.append(m)\n",
    "\n",
    "    for j, probability in enumerate(output):\n",
    "        if probability.shape != (350, 525):\n",
    "            probability = cv2.resize(probability, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n",
    "        probabilities[i * 4 + j, :, :] = probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal values\n",
    "\n",
    "First of all, my thanks to @samusram for finding a mistake in my validation\n",
    "https://www.kaggle.com/c/understanding_cloud_organization/discussion/107711#622412\n",
    "\n",
    "And now I find optimal values separately for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "    threshold   size      dice\n",
      "23       0.35  10000  0.606897\n",
      "26       0.40  10000  0.605114\n",
      "29       0.45  10000  0.604266\n",
      "25       0.40   8000  0.603501\n",
      "28       0.45   8000  0.603165\n",
      "1\n",
      "    threshold   size      dice\n",
      "41       0.65  10000  0.716129\n",
      "52       0.85   8000  0.714070\n",
      "53       0.85  10000  0.713647\n",
      "38       0.60  10000  0.713396\n",
      "44       0.70  10000  0.713016\n",
      "2\n",
      "    threshold   size      dice\n",
      "32       0.50  10000  0.590334\n",
      "29       0.45  10000  0.587162\n",
      "47       0.75  10000  0.587067\n",
      "34       0.55   8000  0.586779\n",
      "35       0.55  10000  0.586186\n",
      "3\n",
      "    threshold   size      dice\n",
      "32       0.50  10000  0.596285\n",
      "29       0.45  10000  0.594540\n",
      "28       0.45   8000  0.593921\n",
      "31       0.50   8000  0.592836\n",
      "26       0.40  10000  0.592549\n"
     ]
    }
   ],
   "source": [
    "class_params = {}\n",
    "for class_id in range(4):\n",
    "    print(class_id)\n",
    "    attempts = []\n",
    "    for t in range(0, 100, 5):\n",
    "        t /= 100\n",
    "        for ms in MIN_SIZE:\n",
    "            masks = []\n",
    "            for i in range(class_id, len(probabilities), 4):\n",
    "                probability = probabilities[i]\n",
    "                predict, num_predict = post_process(sigmoid(probability), t, ms)\n",
    "                masks.append(predict)\n",
    "\n",
    "            d = []\n",
    "            for i, j in zip(masks, valid_masks[class_id::4]):\n",
    "                if (i.sum() == 0) & (j.sum() == 0):\n",
    "                    d.append(1)\n",
    "                else:\n",
    "                    d.append(dice(i, j))\n",
    "\n",
    "            attempts.append((t, ms, np.mean(d)))\n",
    "\n",
    "    attempts_df = pd.DataFrame(attempts, columns=['threshold', 'size', 'dice'])\n",
    "    attempts_df = attempts_df.sort_values('dice', ascending=False)\n",
    "    print(attempts_df.head())\n",
    "    best_threshold = attempts_df['threshold'].values[0]\n",
    "    best_size = attempts_df['size'].values[0]\n",
    "    \n",
    "    class_params[class_id] = (best_threshold, best_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: (0.35, 10000), 1: (0.65, 10000), 2: (0.5, 10000), 3: (0.5, 10000)}\n"
     ]
    }
   ],
   "source": [
    "print(class_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(x='threshold', y='dice', hue='size', data=attempts_df);\n",
    "#plt.title('Threshold and min size vs dice for one of the classes');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at our masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, (input, output) in enumerate(zip(\n",
    "#        valid_dataset, runner.callbacks[0].predictions[\"logits\"])):\n",
    "#    image, mask = input\n",
    "#        \n",
    "#    image_vis = image.transpose(1, 2, 0)\n",
    "#    mask = mask.astype('uint8').transpose(1, 2, 0)\n",
    "#    pr_mask = np.zeros((350, 525, 4))\n",
    "#    for j in range(4):\n",
    "#        probability = cv2.resize(output.transpose(1, 2, 0)[:, :, j], dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n",
    "#        pr_mask[:, :, j], _ = post_process(sigmoid(probability), class_params[j][0], class_params[j][1])\n",
    "    #pr_mask = (sigmoid(output) > best_threshold).astype('uint8').transpose(1, 2, 0)\n",
    "    \n",
    "        \n",
    "#    visualize_with_raw(image=image_vis, mask=pr_mask, original_image=image_vis, original_mask=mask, raw_image=image_vis, raw_mask=output.transpose(1, 2, 0))\n",
    "    \n",
    "#    if i >= 2:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CloudDataset(df=sub, datatype='test', img_ids=test_ids, transforms = get_validation_augmentation(), preprocessing=get_preprocessing(preprocessing_fn))\n",
    "test_loader = DataLoader(test_dataset, batch_size=36, shuffle=False, num_workers=0)\n",
    "\n",
    "loaders = {\"test\": test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [15:40<00:00,  9.13s/it]\n"
     ]
    }
   ],
   "source": [
    "encoded_pixels = []\n",
    "image_id = 0\n",
    "for i, test_batch in enumerate(tqdm.tqdm(loaders['test'])):\n",
    "    runner_out = runner.predict_batch({\"features\": test_batch[0].cuda()})['logits']\n",
    "    for i, batch in enumerate(runner_out):\n",
    "        for probability in batch:\n",
    "            \n",
    "            probability = probability.cpu().detach().numpy()\n",
    "            if probability.shape != (350, 525):\n",
    "                probability = cv2.resize(probability, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n",
    "            predict, num_predict = post_process(sigmoid(probability), class_params[image_id % 4][0], class_params[image_id % 4][1])\n",
    "            if num_predict == 0:\n",
    "                encoded_pixels.append('')\n",
    "            else:\n",
    "                r = mask2rle(predict)\n",
    "                encoded_pixels.append(r)\n",
    "            image_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['EncodedPixels'] = encoded_pixels\n",
    "sub.to_csv('%s_submission.csv'%(datetime.now().strftime(\"%m%d_%H%M%S\")), columns=['Image_Label', 'EncodedPixels'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
